@inproceedings{offenseval,
author = {Zampieri, Marcos and Malmasi, Shervin and Nakov, Preslav and Rosenthal, Sara and Farra, Noura and Kumar, Ritesh},
booktitle = {Proc. 13th Int. Work. Semant. Eval.},
title = {{SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)}},
year = {2019}
}
@inproceedings{OLID,
author = {Zampieri, Marcos and Malmasi, Shervin and Nakov, Preslav and Rosenthal, Sara and Farra, Noura and Kumar, Ritesh},
booktitle = {Proc. NAACL},
title = {{Predicting the Type and Target of Offensive Posts in Social Media}},
year = {2019}
}
@inproceedings{trac2018report,
address = {Santa Fe, USA},
author = {Kumar, Ritesh and Ojha, Atul Kr. and Malmasi, Shervin and Zampieri, Marcos},
booktitle = {Proc. First Work. Trolling, Aggress. Cyberbulling},
title = {{Benchmarking Aggression Identification in Social Media}},
year = {2018}
}
@inproceedings{davidson2017automated,
author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
booktitle = {Proc. ICWSM},
title = {{Automated Hate Speech Detection and the Problem of Offensive Language}},
year = {2017}
}
@inproceedings{dinakar2011modeling,
author = {Dinakar, Karthik and Reichart, Roi and Lieberman, Henry},
booktitle = {Soc. Mob. Web},
pages = {11--17},
title = {{Modeling the detection of Textual Cyberbullying}},
year = {2011}
}
@inproceedings{xu2012learning,
author = {Xu, Jun-Ming and Jun, Kwang-Sung and Zhu, Xiaojin and Bellmore, Amy},
booktitle = {Proc. 2012 Conf. North Am. chapter Assoc. Comput. Linguist. Hum. Lang. Technol.},
organization = {Association for Computational Linguistics},
pages = {656--666},
title = {{Learning from bullying traces in social media}},
year = {2012}
}
@incollection{dadvar2013improving,
author = {Dadvar, Maral and Trieschnigg, Dolf and Ordelman, Roeland and de Jong, Franciska},
booktitle = {Adv. Inf. Retr.},
pages = {693--696},
publisher = {Springer},
title = {{Improving cyberbullying detection with user context}},
year = {2013}
}
@inproceedings{kwok2013locate,
author = {Kwok, Irene and Wang, Yuzhou},
booktitle = {Twenty-Seventh AAAI Conf. Artif. Intell.},
title = {{Locate the hate: Detecting Tweets Against Blacks}},
year = {2013}
}
@article{burnap2015cyber,
author = {Burnap, Pete and Williams, Matthew L},
journal = {Policy {\&} Internet},
number = {2},
pages = {223--242},
publisher = {Wiley Online Library},
title = {{Cyber hate speech on twitter: An application of machine classification and statistical modeling for policy and decision making}},
volume = {7},
year = {2015}
}
@inproceedings{djuric2015hate,
author = {Djuric, Nemanja and Zhou, Jing and Morris, Robin and Grbovic, Mihajlo and Radosavljevic, Vladan and Bhamidipati, Narayan},
booktitle = {Proc. 24th Int. Conf. World Wide Web Companion},
organization = {International World Wide Web Conferences Steering Committee},
pages = {29--30},
title = {{Hate speech detection with comment embeddings}},
year = {2015}
}
@inproceedings{tulkens2016dictionary,
address = {Portoroz, Slovenia},
author = {Tulkens, St{\'{e}}phan and Hilte, Lisa and Lodewyckx, Elise and Verhoeven, Ben and Daelemans, Walter},
booktitle = {Proc. Work. Text Anal. Cybersecurity Online Saf.},
title = {{A Dictionary-based Approach to Racism Detection in Dutch Social Media}},
year = {2016}
}
@inproceedings{ross2016measuring,
address = {Bochum, Germany},
author = {Ross, Bj{\"{o}}rn and Rist, Michael and Carbonell, Guillermo and Cabrera, Benjamin and Kurowsky, Nils and Wojatzki, Michael},
booktitle = {Proc. Work. Nat. Lang. Process. Comput. Commun.},
title = {{Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis}},
year = {2016}
}
@inproceedings{nobata2016abusive,
author = {Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
booktitle = {Proc. 25th Int. Conf. World Wide Web},
organization = {International World Wide Web Conferences Steering Committee},
pages = {145--153},
title = {{Abusive Language Detection in Online User Content}},
year = {2016}
}
@inproceedings{schmidt2017survey,
address = {Valencia, Spain},
author = {Schmidt, Anna and Wiegand, Michael},
booktitle = {Proc. Fifth Int. Work. Nat. Lang. Process. Soc. Media. Assoc. Comput. Linguist.},
pages = {1--10},
title = {{A Survey on Hate Speech Detection Using Natural Language Processing}},
year = {2017}
}
@article{schofield2017identifying,
author = {Schofield, Alexandra and Davidson, Thomas},
journal = {XRDS Crossroads, ACM Mag. Students},
number = {2},
pages = {56--59},
publisher = {ACM},
title = {{Identifying Hate Speech in Social Media}},
volume = {24},
year = {2017}
}
@inproceedings{mubarak2017,
address = {Vancouver, Canada},
author = {Mubarak, Hamdy and Kareem, Darwish and Walid, Magdy},
booktitle = {Proc. Work. Abus. Lang. Online},
title = {{Abusive Language Detection on Arabic Social Media}},
year = {2017}
}
@inproceedings{fiser2017,
address = {Vancouver, Canada},
author = {Fi{\v{s}}er, Darja and Erjavec, Toma{\v{z}} and Ljube{\v{s}}i{\'{c}}, Nikola},
booktitle = {Proc. Work. Work. Abus. Lang. Online},
title = {{Legal Framework, Dataset and Annotation Schema for Socially Unacceptable On-line Discourse Practices in Slovene}},
year = {2017}
}
@inproceedings{su2017,
address = {Vancouver, Canada},
author = {Su, Huei-Po and Huang, Chen-Jie and Chang, Hao-Tsung and Lin, Chuan-Jie},
booktitle = {Proc. Work. Work. Abus. Lang. Online},
title = {{Rephrasing Profanity in Chinese Text}},
year = {2017}
}
@inproceedings{malmasi2017detecting,
author = {Malmasi, Shervin and Zampieri, Marcos},
booktitle = {Proc. Int. Conf. Recent Adv. Nat. Lang. Process.},
pages = {467--472},
title = {{Detecting Hate Speech in Social Media}},
year = {2017}
}
@inproceedings{gamback2017using,
author = {Gamb{\"{a}}ck, Bj{\"{o}}rn and Sikdar, Utpal Kumar},
booktitle = {Proc. First Work. Abus. Lang. Online},
pages = {85--90},
title = {{Using Convolutional Neural Networks to Classify Hate-speech}},
year = {2017}
}
@inproceedings{waseem2017understanding,
author = {Waseem, Zeerak and Davidson, Thomas and Warmsley, Dana and Weber, Ingmar},
booktitle = {Proc. First Work. Abus. Langauge Online},
title = {{Understanding Abuse: A Typology of Abusive Language Detection Subtasks}},
year = {2017}
}
@inproceedings{zhang2018detecting,
author = {Zhang, Ziqi and Robinson, David and Tepper, Jonathan},
booktitle = {Lect. Notes Comput. Sci.},
organization = {Springer Verlag},
title = {{Detecting Hate Speech on Twitter Using a Convolution-GRU Based Deep Neural Network}},
year = {2018}
}
@article{malmasi2018challenges,
author = {Malmasi, Shervin and Zampieri, Marcos},
journal = {J. Exp. Theor. Artif. Intell.},
number = {2},
pages = {1--16},
publisher = {Taylor {\&} Francis},
title = {{Challenges in Discriminating Profanity from Hate Speech}},
volume = {30},
year = {2018}
}
@article{founta2018large,
author = {Founta, Antigoni-Maria and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas},
journal = {arXiv Prepr. arXiv1802.00393},
title = {{Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior}},
year = {2018}
}
@article{fortuna2018survey,
author = {Fortuna, Paula and Nunes, S{\'{e}}rgio},
journal = {ACM Comput. Surv.},
number = {4},
pages = {85},
publisher = {ACM},
title = {{A Survey on Automatic Detection of Hate Speech in Text}},
volume = {51},
year = {2018}
}
@article{elsherief2018hate,
author = {ElSherief, Mai and Kulkarni, Vivek and Nguyen, Dana and Wang, William Yang and Belding, Elizabeth},
journal = {arXiv Prepr. arXiv1804.04257},
title = {{Hate Lingo: A Target-based Linguistic Analysis of Hate Speech in Social Media}},
year = {2018}
}
@inproceedings{wiegand2018overview,
author = {Wiegand, Michael and Siegel, Melanie and Ruppenhofer, Josef},
booktitle = {Proc. GermEval},
title = {{Overview of the GermEval 2018 Shared Task on the Identification of Offensive Language}},
year = {2018}
}
@article{Wolf2019HuggingFacesTS,
author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R'emi and Funtowicz, Morgan and Brew, Jamie},
journal = {ArXiv},
title = {{HuggingFace's Transformers: State-of-the-art Natural Language Processing}},
volume = {abs/1910.0},
year = {2019}
}
@article{roberta,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - Unknown - Roberta A robustly optimized bert pretraining approach.pdf:pdf},
journal = {arxiv.org},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {https://arxiv.org/abs/1907.11692 http://arxiv.org/abs/1907.11692},
year = {2019}
}
@inproceedings{Heist2017,
abstract = {Large-scale knowledge graphs, such as DBpedia, Wikidata, or YAGO, can be enhanced by relation extraction from text, using the data in the knowledge graph as training data, i.e., using distant supervision. While most existing approaches use language-specific methods (usually for English), we present a language-agnostic approach that exploits background knowledge from the graph instead of language-specific techniques and builds machine learning models only from language-independent features. We demonstrate the extraction of relations from Wikipedia abstracts, using the twelve largest language editions of Wikipedia. From those, we can extract 1.6M new relations in DBpedia at a level of precision of 95{\%}, using a RandomForest classifier trained only on language-independent features. Furthermore, we show an exemplary geographical breakdown of the information extracted.},
author = {Heist, Nicolas and Paulheim, Heiko},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-319-68288-4_23},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heist, Conference, 2017 - Unknown - Language-agnostic relation extraction from wikipedia abstracts.pdf:pdf},
isbn = {9783319682877},
issn = {16113349},
pages = {383--399},
title = {{Language-agnostic relation extraction from wikipedia abstracts}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-68288-4{\_}23},
volume = {10587 LNCS},
year = {2017}
}
@inproceedings{BaldiniSoares2019,
abstract = {General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.},
archivePrefix = {arXiv},
arxivId = {1906.03158},
author = {{Baldini Soares}, Livio and FitzGerald, Nicholas and Ling, Jeffrey and Kwiatkowski, Tom},
booktitle = {arxiv.org},
doi = {10.18653/v1/p19-1279},
eprint = {1906.03158},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldini Soares et al. - 2019 - Matching the Blanks Distributional Similarity for Relation Learning.pdf:pdf},
pages = {2895--2905},
title = {{Matching the Blanks: Distributional Similarity for Relation Learning}},
url = {https://arxiv.org/abs/1906.03158},
year = {2019}
}
@inproceedings{attention,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Adv. Neural Inf. Process. Syst.},
eprint = {1706.03762},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention is all you need.pdf:pdf},
issn = {10495258},
pages = {5999--6009},
title = {{Attention is all you need}},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need},
volume = {2017-Decem},
year = {2017}
}
@book{Pustejovsky2013,
abstract = {Create your own natural language training corpus for machine learning. Whether you're working with English, Chinese, or any other natural language, this hands-on book guides you through a proven annotation development cycle—the process of adding metadata to your training corpus to help ML algorithms work more efficiently. You don't need any programming or linguistics experience to get started. Using detailed examples at every step, you'll learn how the MATTER Annotation Development Process helps you Model, Annotate, Train, Test, Evaluate, and Revise your training corpus. You also get a complete walkthrough of a real-world annotation project. Define a clear annotation goal before collecting your dataset (corpus) Learn tools for analyzing the linguistic content of your corpus Build a model and specification for your annotation project Examine the different annotation formats, from basic XML to the Linguistic Annotation Framework Create a gold standard corpus that can be used to train and test ML algorithms Select the ML algorithms that will process your annotated data Evaluate the test results and revise your annotation task Learn how to use lightweight software for annotating texts and adjudicating the annotations This book is a perfect companion to O'Reilly's Natural Language Processing with Python.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Pustejovsky, James and Stubbs, Amber},
booktitle = {Vasa},
doi = {1332788036},
eprint = {arXiv:1011.1669v3},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pustejovsky, Stubbs - 2013 - Natural language annotation for machine learning.pdf:pdf},
isbn = {9781449306663},
issn = {1098-6596},
pages = {1--97},
pmid = {25246403},
title = {{Natural language annotation for machine learning}},
url = {www.it-ebooks.info http://it-ebooks.info/book/681/{\%}5Cnpapers3://publication/uuid/906A922E-DE39-4CAB-8067-F222D065ACEF},
year = {2013}
}
@inproceedings{Le2019,
author = {Le, T A and Petrov, M A and Kurato, Y. M. and Burtsev, M S},
booktitle = {Comput. Linguist. Intellect. Technol. Pap. from Annu. Int. Conf. “Dialogue”},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le et al. - 2019 - Sentence Level Representation and Language Models in The Task of Coreference Resolution for Russian.pdf:pdf},
pages = {341--350},
title = {{Sentence Level Representation and Language Models in The Task of Coreference Resolution for Russian}},
year = {2019}
}
@inproceedings{tacred,
abstract = {Organized relational knowledge in the form of “knowledge graphs” is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset, obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2{\%} to 26.7{\%}.},
author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D},
booktitle = {EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc.},
doi = {10.18653/v1/d17-1004},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Position-aware attention and supervised data improve slot filling.pdf:pdf},
isbn = {9781945626838},
pages = {35--45},
title = {{Position-aware attention and supervised data improve slot filling}},
year = {2017}
}
@techreport{Zhang,
abstract = {Organized relational knowledge in the form of "knowledge graphs" is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset, obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F 1 score increases markedly from 22.2{\%} to 26.7{\%}.},
author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Position-aware Attention and Supervised Data Improve Slot Filling.pdf:pdf},
pages = {35--45},
title = {{Position-aware Attention and Supervised Data Improve Slot Filling}}
}
@techreport{spanbert,
abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Span-BERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT large , our single model obtains 94.6{\%} and 88.7{\%} F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6{\%} F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE. 1},
archivePrefix = {arXiv},
arxivId = {1907.10529v3},
author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer and Allen, †},
eprint = {1907.10529v3},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joshi et al. - Unknown - SpanBERT Improving Pre-training by Representing and Predicting Spans.pdf:pdf},
title = {{SpanBERT: Improving Pre-training by Representing and Predicting Spans}},
url = {https://github.com/facebookresearch/}
}
@article{bert,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@article{ontonotes,
abstract = {A forged and round-rolled pure tantalum bar stock was observed to exhibit large asymmetry in bulk plastic flow response when subjected to large strain Taylor cylinder impact testing. This low-symmetry behavior was analyzed experimentally investigating both the initial stock and the impact-deformed material via x-ray crystallographic texture measurements and automated electron back-scatter diffraction scans to establish spatial microstructural uniformity. Polycrystal simulations based upon the (I 10) measured duplex texture and experimentally inferred deformation mechanisms were performed to project discrete yield surface shapes. Subsequent least squares fitting and eigensystem analysis of the resulting quadratic fourth-order tensors revealed strong normal/shear stress coupling in the yield surface shape. This mixed-mode coupling produces a shearing deformation in the 1-2 impact plane of a Taylor specimen whose axis is coincident with the compressive 3-axis. The resultant deformation generates an unusual rectangular-shaped impact footprint that is confirmed by finite-element calculations compared to experimental post-test geometries. (C) 2002 Elsevier Science Ltd. All rights reserved.},
author = {et Al, Ralph Weischedel},
isbn = {0749-6419},
journal = {Linguist. Data Consort.},
title = {{OntoNotes Release 5.0 LDC2013T19}},
year = {2013}
}
@inproceedings{freebase,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Free-base currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
booktitle = {Proc. ACM SIGMOD Int. Conf. Manag. Data},
doi = {10.1145/1376616.1376746},
isbn = {9781605581026},
issn = {07308078},
keywords = {Design,Human factors,Languages},
pages = {1247--1249},
title = {{Freebase: A collaboratively created graph database for structuring human knowledge}},
year = {2008}
}
@inproceedings{factrueval16,
abstract = {In this paper, we describe the rules and results of the FactRuEval information extraction competition held in 2016 as part of the Dialogue Evaluation initiative in the run-up to Dialogue 2016. The systems were to extract information from Russian texts and competed in two named entity extraction tracks and one fact extraction track. The paper describes the tasks set before the participants and presents the scores achieved by the contending systems. Additionally, we dwell upon the scoring methods employed for evaluating the results of all the three tracks and provide some preliminary analysis of the state of the art in Information Extraction for Russian texts. We also provide a detailed description of the composition and general organization of the annotated corpus created for the competition by volunteers using the OpenCorpora.org platform. The corpus is publicly available and is expected to evolve in the future.},
author = {Starostin, A. S. and Bocharov, V. V. and Alexeeva, S. V. and Bodrova, A. A. and Chuchunkov, A. S. and Dzhumaev, S. S. and Efimenko, I. V. and Granovsky, D. V. and Khoroshevsky, V. F. and Krylova, I. V. and Nikolaeva, M. A. and Smurov, I. M. and Toldova, S. Y.},
booktitle = {Komp'juternaja Lingvistika i Intellektual'nye Tehnol.},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Starostin et al. - 2016 - FactRuEval 2016 Evaluation of named entity recognition and fact extraction systems for Russian.pdf:pdf},
issn = {20757182},
keywords = {Evaluation,Fact extraction,Information extraction,Named entity recognition,Relation extraction},
pages = {702--720},
title = {{FactRuEval 2016: Evaluation of named entity recognition and fact extraction systems for Russian}},
year = {2016}
}
@inproceedings{nyt10,
abstract = {Several recent works on relation extraction have been applying the distant supervision paradigm: instead of relying on annotated text to learn how to predict relations, they employ existing knowledge bases (KBs) as source of supervision. Crucially, these approaches are trained based on the assumption that each sentence which mentions the two related entities is an expression of the given relation. Here we argue that this leads to noisy patterns that hurt precision, in particular if the knowledge base is not directly related to the text we are working with. We present a novel approach to distant supervision that can alleviate this problem based on the following two ideas: First, we use a factor graph to explicitly model the decision whether two entities are related, and the decision whether this relation is mentioned in a given sentence; second, we apply constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in our training KB. We apply our approach to extract relations from the New York Times corpus and use Freebase as knowledge base. When compared to a state-of-the-art approach for relation extraction under distant supervision, we achieve 31{\%} error reduction. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
author = {Riedel, Sebastian and Yao, Limin and McCallum, Andrew},
booktitle = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
doi = {10.1007/978-3-642-15939-8_10},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riedel, Yao, McCallum - 2010 - Modeling relations and their mentions without labeled text.pdf:pdf},
isbn = {3642159389},
issn = {03029743},
number = {PART 3},
pages = {148--163},
title = {{Modeling relations and their mentions without labeled text}},
volume = {6323 LNAI},
year = {2010}
}
@inproceedings{distant-supervision,
abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE- style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of enti- ties that appears in some Freebase relation, we find all sentences containing those entities in a large un- labeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 re- lations at a precision of 67.6{\%}. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.},
author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
doi = {10.3115/1690219.1690287},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mintz et al. - 2009 - Distant supervision for relation extraction without labeled data.pdf:pdf},
pages = {1003--1011},
title = {{Distant supervision for relation extraction without labeled data}},
year = {2009}
}
@inproceedings{min-etal-2013-distant,
address = {Atlanta, Georgia},
author = {Min, Bonan and Grishman, Ralph and Wan, Li and Wang, Chang and Gondek, David},
booktitle = {Proc. 2013 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.},
pages = {777--782},
publisher = {Association for Computational Linguistics},
title = {{Distant Supervision for Relation Extraction with an Incomplete Knowledge Base}},
url = {https://www.aclweb.org/anthology/N13-1095},
year = {2013}
}
@article{distre,
abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6{\%} and 88.7{\%} F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6$\backslash${\%} F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.},
address = {Florence, Italy},
archivePrefix = {arXiv},
arxivId = {1907.10529},
author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
eprint = {1907.10529},
journal = {Proc. 57th Annu. Meet. Assoc. Comput. Linguist.},
pages = {1388--1398},
publisher = {Association for Computational Linguistics},
title = {{SpanBERT: Improving Pre-training by Representing and Predicting Spans}},
url = {https://www.aclweb.org/anthology/P19-1134 http://arxiv.org/abs/1907.10529},
year = {2019}
}
@inproceedings{deeppavlov,
abstract = {Adoption of messaging communication and voice assistants has grown rapidly in the last years. This creates a demand for tools that speed up prototyping of featurerich dialogue systems. An open-source library DeepPavlov is tailored for development of conversational agents. The library prioritises efficiency, modularity, and extensibility with the goal to make it easier to develop dialogue systems from scratch and with limited data available. It supports modular as well as end-to-end approaches to implementation of conversational agents. Conversational agent consists of skills and every skill can be decomposed into components. Components are usually models which solve typical NLP tasks such as intent classification, named entity recognition or pre-trained word vectors. Sequence-to-sequence chit-chat skill, question answering skill or task-oriented skill can be assembled from components provided in the library.},
author = {Burtsev, Mikhail and Seliverstov, Alexander and Airapetyan, Rafael and Arkhipov, Mikhail and Baymurzina, Dilyara and Bushkov, Nickolay and Gureenkova, Olga and Khakhulin, Taras and Kuratov, Yuri and Kuznetsov, Denis and Litinsky, Alexey and Logacheva, Varvara and Lymar, Alexey and Malykh, Valentin and Petrov, Maxim and Polulyakh, Vadim and Pugachev, Leonid and Sorokin, Alexey and Vikhreva, Maria and Zaynutdinov, Marat},
booktitle = {ACL 2018 - 56th Annu. Meet. Assoc. Comput. Linguist. Proc. Syst. Demonstr.},
doi = {10.18653/v1/p18-4021},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burtsev et al. - 2018 - DeepPavlov Open-Source library for dialogue systems.pdf:pdf},
isbn = {9781948087650},
pages = {122--127},
title = {{DeepPavlov: Open-Source library for dialogue systems}},
url = {https://github.com/deepmipt/},
year = {2018}
}
@inproceedings{Modi:12,
address = {Montr{\'{e}}al, QC, Canada},
author = {Modi, Ashutosh and Titov, Ivan and Klementiev, Alexandre},
booktitle = {Proc. NAACL-HLT Work. Induction Linguist. Struct.},
pages = {1--7},
publisher = {Association for Computational Linguistics},
series = {WILS{\~{}}2012},
title = {{Unsupervised Induction of Frame-Semantic Representations}},
url = {https://aclweb.org/anthology/W12-1901},
year = {2012}
}
@inproceedings{kallmeyer2018coarse,
author = {Kallmeyer, Laura and QasemiZadeh, Behrang and Cheung, Jackie Chi Kit},
booktitle = {Proc. Seventh Jt. Conf. Lex. Comput. Semant.},
pages = {130--141},
title = {{Coarse Lexical Frame Acquisition at the Syntax--Semantics Interface Using a Latent-Variable PCFG Model}},
year = {2018}
}
@inproceedings{D18-1523,
author = {Amrami, Asaf and Goldberg, Yoav},
booktitle = {Proc. 2018 Conf. Empir. Methods Nat. Lang. Process.},
pages = {4860--4867},
publisher = {Association for Computational Linguistics},
title = {{Word Sense Induction with Neural biLM and Symmetric Patterns}},
url = {http://aclweb.org/anthology/D18-1523},
year = {2018}
}
@article{NIPS2017_7181,
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762},
journal = {CoRR},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
volume = {abs/1706.0},
year = {2017}
}
@inproceedings{P18-2010,
abstract = {We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.},
address = {Melbourne, Australia},
author = {Ustalov, Dmitry and Panchenko, Alexander and Kutuzov, Andrey and Biemann, Chris and Ponzetto, Simone Paolo},
booktitle = {Proc. 56th Annu. Meet. Assoc. Comput. Linguist. (Volume 2 Short Pap.},
pages = {55--62},
publisher = {Association for Computational Linguistics},
title = {{Unsupervised Semantic Frame Induction using Triclustering}},
url = {https://www.aclweb.org/anthology/P18-2010},
year = {2018}
}
@article{DBLP:journals/corr/abs-1810-04805,
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
journal = {CoRR},
title = {{{\{}BERT:{\}} Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
volume = {abs/1810.0},
year = {2018}
}
@article{joshi2019spanbert,
author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
journal = {arXiv Prepr. arXiv1907.10529},
title = {{Spanbert: Improving pre-training by representing and predicting spans}},
year = {2019}
}
@inproceedings{struyansky_arefyev_18,
author = {Struyanskiy, Oleg and Arefyev, Nikolay},
booktitle = {Suppl. Proc. Seventh Int. Conf. Anal. Images, Soc. Networks Texts (AIST 2018)},
pages = {208--213},
title = {{Neural {\{}Networks{\}} with {\{}Attention{\}} for {\{}Word Sense Induction{\}}}},
url = {http://ceur-ws.org/Vol-2268/paper23.pdf},
year = {2018}
}
@inproceedings{Panchenko2012ASS,
author = {Panchenko, Alexander and Morozova, Olga and Naets, Hubert},
booktitle = {KONVENS},
title = {{A semantic similarity measure based on lexico-syntactic patterns}},
url = {https://pdfs.semanticscholar.org/c9f5/94202b5863d0303e32f7ee777b1c0cff5d1b.pdf?{\_}ga=2.136315699.2060908634.1550078587-109422695.1550078587},
year = {2012}
}
@inproceedings{Materna:13,
address = {Atlanta, GA, USA},
author = {Materna, Ji$\backslash$vr{\'{i}}},
booktitle = {Proc. 2013 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.},
pages = {482--486},
publisher = {Association for Computational Linguistics},
series = {NAACL-HLT{\~{}}2013},
title = {{Parameter Estimation for LDA-Frames}},
url = {https://aclweb.org/anthology/N13-1051},
year = {2013}
}
@inproceedings{zhang2017tacred,
author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D},
booktitle = {Proc. 2017 Conf. Empir. Methods Nat. Lang. Process. (EMNLP 2017)},
pages = {35--45},
title = {{Position-aware Attention and Supervised Data Improve Slot Filling}},
url = {https://nlp.stanford.edu/pubs/zhang2017tacred.pdf},
year = {2017}
}
@techreport{OConnor:13,
address = {Pittsburgh, PA, USA},
author = {O'Connor, Brendan},
institution = {Machine Learning Department, Carnegie Mellon University},
title = {{Learning Frames from Text with an Unsupervised Latent Variable Model}},
url = {https://arxiv.org/abs/1307.7382},
year = {2013}
}
@inproceedings{behrang_etal_19,
author = {QasemiZadeh, Behrang and Petruck, Miriam R L and Stodden, Regina and Kallmeyer, Laura and Candito, Marie},
booktitle = {Proc. 13th Int. Work. Semant. Eval.},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2019 Task 2: Unsupervised Lexical Frame Induction}},
year = {2019}
}
@inproceedings{brat,
address = {Avignon, France},
author = {Stenetorp, Pontus and Pyysalo, Sampo and Topi{\'{c}}, Goran and Ohta, Tomoko and Ananiadou, Sophia and Tsujii, Jun'ichi},
booktitle = {Proc. Demonstr. Sess. EACL 2012},
keywords = {brat},
mendeley-tags = {brat},
publisher = {Association for Computational Linguistics},
title = {{brat: a Web-based Tool for NLP-Assisted Text Annotation}},
year = {2012}
}
@inproceedings{panchenko_etal_18,
author = {Panchenko, Alexander and Lopukhina, Anastasiya and Ustalov, Dmitry and Lopukhin, Konstantin and Arefyev, Nikolay and Leontyev, Alexey and Loukachevitch, Natalia V},
booktitle = {Comput. Linguist. Intellect. Technol. Pap. from Annu. Int. Conf. “Dialogue”},
pages = {547--564},
publisher = {RSUH},
title = {{RUSSE'2018: {\{}A{\}} Shared Task on Word Sense Induction for the Russian Language}},
url = {http://arxiv.org/abs/1803.05795},
year = {2018}
}
@inproceedings{Cheung:13,
address = {Atlanta, GA, USA},
author = {Cheung, Jackie C K and Poon, Hoifung and Vanderwende, Lucy},
booktitle = {Proc. 2013 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.},
pages = {837--846},
publisher = {Association for Computational Linguistics},
series = {NAACL-HLT{\~{}}2013},
title = {{Probabilistic Frame Induction}},
url = {https://aclweb.org/anthology/N13-1104},
year = {2013}
}
@inproceedings{DBLP:journals/corr/abs-1805-09209,
author = {Arefyev, Nikolay and Ermolaev, Pavel and Panchenko, Alexander},
booktitle = {Comput. Linguist. Intellect. Technol. Pap. from Annu. Int. Conf. “Dialogue”},
pages = {68--84},
publisher = {RSUH},
title = {{How much does a word weigh? Weighting word embeddings for word sense induction}},
url = {http://arxiv.org/abs/1805.09209},
year = {2018}
}
@inproceedings{N18-1202,
author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
booktitle = {Proc. 2018 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. Vol. 1 (Long Pap.},
doi = {10.18653/v1/N18-1202},
pages = {2227--2237},
publisher = {Association for Computational Linguistics},
title = {{Deep Contextualized Word Representations}},
url = {http://aclweb.org/anthology/N18-1202},
year = {2018}
}
@inproceedings{S13-2050,
author = {Baskaya, Osman and Sert, Enis and Cirik, Volkan and Yuret, Deniz},
booktitle = {Second Jt. Conf. Lex. Comput. Semant. (*SEM), Vol. 2 Proc. Seventh Int. Work. Semant. Eval. (SemEval 2013)},
pages = {300--306},
publisher = {Association for Computational Linguistics},
title = {{AI-KU: Using Substitute Vectors and Co-Occurrence Modeling For Word Sense Induction and Disambiguation}},
url = {http://aclweb.org/anthology/S13-2050},
year = {2013}
}
@inproceedings{K15-1026,
author = {Schwartz, Roy and Reichart, Roi and Rappoport, Ari},
booktitle = {Proc. Ninet. Conf. Comput. Nat. Lang. Learn.},
doi = {10.18653/v1/K15-1026},
pages = {258--267},
publisher = {Association for Computational Linguistics},
title = {{Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction}},
url = {http://aclweb.org/anthology/K15-1026},
year = {2015}
}
@inproceedings{Kawahara:14,
address = {Baltimore, MD, USA},
author = {Kawahara, Daisuke and Peterson, Daniel W and Palmer, Martha},
booktitle = {Proc. 52nd Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap.},
pages = {1030--1040},
publisher = {Association for Computational Linguistics},
series = {ACL{\~{}}2014},
title = {{A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes}},
url = {https://aclweb.org/anthology/P14-1097},
year = {2014}
}
@inproceedings{S13-2049,
author = {Jurgens, David and Klapaftis, Ioannis},
booktitle = {Second Jt. Conf. Lex. Comput. Semant. (*SEM), Vol. 2 Proc. Seventh Int. Work. Semant. Eval. (SemEval 2013)},
pages = {290--299},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses}},
url = {http://aclweb.org/anthology/S13-2049},
year = {2013}
}
@inproceedings{C02-1114,
author = {Widdows, Dominic and Dorow, Beate},
booktitle = {COLING 2002 19th Int. Conf. Comput. Linguist.},
title = {{A Graph Model for Unsupervised Lexical Acquisition}},
url = {http://aclweb.org/anthology/C02-1114},
year = {2002}
}
@incollection{NIPS2017_7181_2,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Adv. Neural Inf. Process. Syst. 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
pages = {5998--6008},
publisher = {Curran Associates, Inc.},
title = {{Attention is {\{}All{\}} you {\{}Need{\}}}},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
year = {2017}
}
@inproceedings{Hearst1992,
abstract = {We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.},
annote = {Muito referenciado. Parece ter padr{\{}{\~{o}}{\}}es uteis.},
author = {Hearst, Marti A},
booktitle = {Proc. 14th Conf. Comput. Linguist.},
doi = {10.3115/992133.992154},
keywords = {semantic relations},
mendeley-tags = {semantic relations},
number = {July},
organization = {Association for Computational Linguistics Morristown, NJ, USA},
pages = {539--545},
publisher = {Association for Computational Linguistics},
series = {COLING '92},
title = {{Automatic acquisition of hyponyms from large text corpora}},
url = {http://portal.acm.org/citation.cfm?doid=992133.992154},
volume = {II},
year = {1992}
}
@article{Pedregosa2011,
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
pages = {2825--2830},
publisher = {JMLR. org},
title = {{Scikit-learn: Machine learning in Python}},
volume = {12},
year = {2011}
}
